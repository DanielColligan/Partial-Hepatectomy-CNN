# -*- coding: utf-8 -*-
"""Unet2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zJNKcAGjuGHuGdnlIMSmeCcB4anh5153

# Setup
"""

!pip install -q git+https://github.com/tensorflow/examples.git
!pip install tensorflow_io

# Importing Libraries
import os
import tensorflow
from tensorflow_examples.models.pix2pix import pix2pix
from IPython.display import clear_output
import matplotlib.pyplot as pyplot
import tensorflow_io as tensorflowio
import numpy
from sklearn.model_selection import KFold
import seaborn

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

def decode_image(filename, label):
        def load_image(f):
            try:  
                string = tensorflow.io.read_file(f)
            except Exception as e:
                print(f)
                raise e
            decoded = tensorflowio.experimental.image.decode_tiff(string, index=0) # converts to RGBA
            without_alpha = decoded[:,:,:3] # converts to rgb
            reshaped = tensorflow.reshape(without_alpha, [1, 420, 580, 3]) # formatted in expected input format for unet
            return tensorflow.cast(reshaped, tensorflow.float32) # returns image
        image = load_image(filename)
        mask = load_image(label)
        return image, mask

# Sets file path constants
data_path = r'/content/drive/MyDrive/Uni Team Project Computer Vision/train'
paths = os.listdir(data_path)
cache_file_path = os.path.join(data_path, 'cache', 'cache.txt')

def create_pair_paths():
    if USE_CACHE == True:
        with open(cache_file_path, "r") as f: # put into tuple format
            cached_pair_paths = f.read()
            cached_pair_path_items = [tuple(item.split(",")) for item in cached_pair_paths.split("\n")]
            pair_paths = cached_pair_path_items
    else:
        mask_paths = [i for i in paths if 'mask' in i] # create list of masks
        image_paths = [i for i in paths if 'mask' not in i] # create list of images
        pair_paths = []

        for image_path in image_paths: # create expected mask file name
            mask_path = image_path.replace('.tif', '')
            mask_path += '_mask.tif'
            if mask_path in mask_paths: # checks if expected mask file name extsts, if exists, create absolute file path
                image_path = os.path.join(data_path, image_path)
                mask_path = os.path.join(data_path, mask_path)
                pair_paths.append((image_path, mask_path)) # adds to dataset as tuple
    return pair_paths

def pair_path_split(pair_paths):
    image_paths = [i[0] for i in pair_paths]
    mask_paths = [i[1] for i in pair_paths]
    return image_paths, mask_paths

def normalise(input_image, input_mask):
    # sets value between 0 and 1
    input_image = tensorflow.cast(tensorflow.math.abs(input_image), tensorflow.float32) / 255.0
    input_mask = tensorflow.cast(tensorflow.math.abs(input_mask), tensorflow.float32) / 255.0
    # format image and mask tensor shape into expected format
    input_image = input_image[:, :, :, :INPUT_CHANNELS]
    input_mask = input_mask[:, :, :, :FILTERS]
    # ensures categoric 0 or 1 for mask
    input_mask = tensorflow.math.round(input_mask)
    return input_image, input_mask

def resize(input_image, input_mask):
    # resize image to IMAGE_SIZE*IMAGE_SIZE
    input_image = tensorflow.image.resize(input_image, (IMAGE_SIZE, IMAGE_SIZE))
    input_mask = tensorflow.image.resize(input_mask, (IMAGE_SIZE, IMAGE_SIZE))
    return input_image, input_mask

def flip(input_image, input_mask, left_right, up_down):
    if left_right == True: # vertical flip
        input_image =tensorflow.image.flip_left_right(input_image)
        input_mask = tensorflow.image.flip_left_right(input_mask)
    if up_down == True: # horizontal flip
        input_image =tensorflow.image.flip_up_down(input_image)
        input_mask = tensorflow.image.flip_up_down(input_mask)
    return input_image, input_mask

def rotate(input_image, input_mask, angle):
    # rotate image by provided angle
    input_image = tensorflow.image.rot90(input_image, k=(angle//3))
    input_mask = tensorflow.image.rot90(input_mask, k=(angle//3))
    return input_image, input_mask

def load_image_training(index, input_items):
    # separate tuple
    input_image, input_mask = input_items
    i = index // len(paths) # puts each data entry into category 1-16
    horizontal = i > 8 # final 8 horizontal flip
    vertical = (((i // 4) % 2) == 0) # vertical flip in groups of 4
    angle = tensorflow.cast((i % 4) * 90, tensorflow.int32) # iteratively assigns angle
    # applies transorfmations based off above parameters
    input_image, input_mask = resize(input_image, input_mask)
    input_image, input_mask = flip(input_image, input_mask, horizontal, vertical)
    input_image, input_mask = rotate(input_image, input_mask, angle)
    input_image, input_mask = normalise(input_image, input_mask)
    return input_image, input_mask

def create_datasets(paths):
    split_paths = pair_path_split(paths)
    dataset = tensorflow.data.Dataset.from_tensor_slices(split_paths) # creates datasets
    dataset = dataset.map(decode_image)
    dataset = dataset.repeat(16) # repeat data for data expansion
    dataset = dataset.enumerate(start=0) # adds index
    dataset = dataset.map(load_image_training, num_parallel_calls=tensorflow.data.AUTOTUNE)
    dataset = dataset.shuffle(len(dataset), reshuffle_each_iteration=True) # shuffle dataset
    return dataset

def display_tif(display_list):
    if len(display_list) == 2: # sets title for images based off inputs
        pyplot.figure(figsize=(12, 10))
        title = ['Input Image', 'Real Mask', 'Real Superimposed']
    else:
        pyplot.figure(figsize=(20, 10))
        title = ['Input Image', 'Real Mask', 'Predicted Mask', 
                'Real Superimposed', 'Predicted Superimposed']
    # superimposes image by halving each value in matrix and summing them to get overall image
    input_blend = tensorflow.math.scalar_mul(0.5, display_list[0][..., :1])
    real_blend = tensorflow.math.scalar_mul(0.5, display_list[1][..., :1])
    real_superimposed = tensorflow.math.add(input_blend, real_blend)
    display_list.append(real_superimposed)
    if (len(display_list)-1) == 3: # if required, repeat for predicted masks
        pred_blend = tensorflow.math.scalar_mul(0.5, display_list[2][..., :1])
        pred_superimposed = tensorflow.math.add(input_blend, pred_blend)
        display_list.append(pred_superimposed)
    # display all images in list
    for i in range(len(display_list)):
        pyplot.subplot(1, len(display_list), i+1)
        pyplot.title(title[i])
        image_out = display_list[i][..., :1] # ensures correct formatting, changing input channels could break without this
        reshape = tensorflow.reshape(image_out, (IMAGE_SIZE, IMAGE_SIZE, 1))
        rgb = tensorflow.image.grayscale_to_rgb(reshape)
        pyplot.imshow(tensorflow.keras.preprocessing.image.array_to_img(rgb))
        pyplot.axis('off')
    pyplot.show()

"""# Define the Model"""

def create_down_stack():
    base_model = tensorflow.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)
    # Use the activations of these layers
    layer_names = [
        'block_1_expand_relu',   # 64x64
        'block_3_expand_relu',   # 32x32
        'block_6_expand_relu',   # 16x16
        'block_13_expand_relu',  # 8x8
        'block_16_project']      # 4x4
    base_model_outputs = [base_model.get_layer(name).output for name in layer_names]
    # Create the feature extraction model
    down_stack = tensorflow.keras.Model(inputs=base_model.input, outputs=base_model_outputs)
    down_stack.trainable = False
    return down_stack

def create_up_stack():
    up_stack = [
        pix2pix.upsample(512, 3),  # 4x4 -> 8x8
        pix2pix.upsample(256, 3),  # 8x8 -> 16x16
        pix2pix.upsample(128, 3),  # 16x16 -> 32x32
        pix2pix.upsample(64, 3)]   # 32x32 -> 64x64
    return up_stack

def unet_model(filters):
    inputs = tensorflow.keras.layers.Input(shape=[128, 128, 3])
    down_stack = create_down_stack()
    up_stack = create_up_stack()
    # Downsampling through the model
    skips = down_stack(inputs)
    x = skips[-1]
    skips = reversed(skips[:-1])
    # Upsampling and establishing the skip connections
    for up, skip in zip(up_stack, skips):
        x = up(x)
        concat = tensorflow.keras.layers.Concatenate()
        x = concat([x, skip])
    # This is the last layer of the model
    last = tensorflow.keras.layers.Conv2DTranspose(filters, 3, strides=2,padding='same', activation='relu')  #64x64 -> 128x128
    x = last(x)
    return tensorflow.keras.Model(inputs, x)

def true_positive(y_true, y_pred):
    y_pred = tensorflow.math.minimum(y_pred, tensorflow.constant([1.])) # ensures cell range 0-1 used throughout
    # x/2 as currently 2 channels, twice as many as pixels
    return tensorflow.math.reduce_sum(y_true*y_pred)/2 # multiply means only 1 and 1 returns 1

def true_negative(y_true, y_pred):
    y_pred = tensorflow.math.minimum(y_pred, tensorflow.constant([1.]))
    union = tensorflow.math.reduce_sum(y_true+y_pred) # True OR Predicted
    return ((128*128*2)-union)/2 # 1 - OR = (0 AND 0)

def false_positive(y_true, y_pred):
    y_pred = tensorflow.math.minimum(y_pred, tensorflow.constant([1.]))
    tp = true_positive(y_true, y_pred)
    pred_ones = tensorflow.math.reduce_sum(y_pred)/2 # All 1s in predicted mask
    return pred_ones - tp # pred1s - actual1s = fp

def false_negative(y_true, y_pred):
    y_pred = tensorflow.math.minimum(y_pred, tensorflow.constant([1.]))
    tp = true_positive(y_true, y_pred)
    true_ones = tensorflow.math.reduce_sum(y_true)/2 # Alll 1s in true mask
    return true_ones - tp # true1s - actual1s = fn

def recall(y_true, y_pred):
    tp = true_positive(y_true, y_pred)
    fn = false_negative(y_true, y_pred)
    return (tp+1)/(tp+fn+1) # included smoothing to recall formula

def precision(y_true, y_pred):
    tp = true_positive(y_true, y_pred)
    fp = false_positive(y_true, y_pred)
    return (tp+1)/(tp+fp+1) # smoothing added to precision formula

def dice(y_true, y_pred):
    y_pred = tensorflow.math.minimum(y_pred, tensorflow.constant([1.]))
    intersection  = tensorflow.math.reduce_sum(y_true*y_pred)
    union = tensorflow.math.reduce_sum(y_true+y_pred)
    return (2*intersection + 1) / (union + 1)

def dice_BCE_coef_loss(y_true, y_pred):
    bce_weight = 0.5
    dice_coef = dice(y_true, y_pred)
    dice_loss = 1 - dice_coef
    BCE_func =  tensorflow.keras.losses.BinaryCrossentropy(from_logits=True)
    BCE = BCE_func(y_true, y_pred)
    return (BCE * bce_weight) + (dice_loss * (1 - bce_weight))

"""# Train Model"""

def create_mask(predicted_mask):
    predicted_mask = tensorflow.argmax(predicted_mask, axis=-1)
    predicted_mask = predicted_mask[..., tensorflow.newaxis]
    predicted_mask = tensorflow.image.grayscale_to_rgb(predicted_mask)
    return predicted_mask[0]

def create_y_pred(image):
        predicted_mask = create_mask(model.predict(image[...]))
        pred_mask = tensorflow.cast(predicted_mask, tensorflow.float32)
        return pred_mask[tensorflow.newaxis, :, :, :2]

def show_predictions(dataset=None, num=1):
    if dataset:
        for image, mask in dataset.take(num):
            display_tif([image, mask,create_y_pred(image)])
    else:
        display_tif([sample_image, sample_mask, create_y_pred(sample_image)])

def lr_scheduler(epoch, lr): # learning rate decay function
    if epoch < 10:
        return lr
    else:
        return lr*DECAY

class DisplayCallbacks(tensorflow.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        clear_output(wait=True)
        print ('KFold Loop: {}    '.format(kfold_loop),
               'Sample Prediction after epoch {}    '.format(epoch+1),
               'Learning Rate: {}'.format(LEARNING_RATE))
        show_predictions() # display predictions

def summarise_diagnostics(histories):
    for i in range(len(histories)):
        loss = histories[i].history['loss']
        val_loss = histories[i].history['val_loss']
        acc = histories[i].history['accuracy']
        val_acc = histories[i].history['val_accuracy']
        dice = histories[i].history['dice']
        val_dice = histories[i].history['val_dice']
        epochs = range(len(loss))

        fig, (ax1, ax2, ax3, ax4) = pyplot.subplots(1, 4, figsize=(25,5))
        #fig.suptitle('KFold {}, Learning Rate {}'.format(i+1, LEARNING_RATE), fontsize=20)
        fig.suptitle('KFold {}'.format(i+1), fontsize=20)
        # plot loss
        ax1.set_title('Loss', fontsize=18)
        ax1.plot(epochs, loss, color='blue', label='train')
        ax1.plot(epochs, val_loss, color='red', label='test')
        ax1.legend(loc='upper right')
        ax1.set_ylabel('Loss', fontsize=16)
        ax1.set_xlabel('Epoch', fontsize=16)
        # plot accuracy
        ax2.set_title('Accuracy', fontsize=18)
        ax2.plot(epochs, acc, color='blue', label='train')
        ax2.plot(epochs, val_acc, color='red', label='test')
        ax2.legend(loc='upper right')
        ax2.set_ylabel('Accuracy', fontsize=16)
        ax2.set_xlabel('Epoch', fontsize=16)
        # plot dice
        ax3.set_title('Dice Score', fontsize=18)
        ax3.plot(epochs, dice, color='blue', label='train')
        ax3.plot(epochs, val_dice, color='red', label='test')
        ax3.legend(loc='upper right')
        ax3.set_ylabel('Dice Score', fontsize=16)
        ax3.set_xlabel('Epoch', fontsize=16)
        # plot confusion matrix
        tp = int(histories[i].history['true_positive'][-1])
        tn = int(histories[i].history['true_negative'][-1])
        fp = int(histories[i].history['false_positive'][-1])
        fn = int(histories[i].history['false_negative'][-1])
        sum = tp+tn+fp+fn
        # create confusion matrix for annotation
        confusion_matrix = [['True Positive \n {} \n {:.2f}%'.format(int(tp), 100*tp/sum), 
                            'False Negative \n {} \n {:.2f}%'.format(int(fn), 100*fn/sum)],
                            ['False Positive \n {} \n {:.2f}%'.format(int(fp), 100*fp/sum), 
                            'True Negative \n {} \n {:.2f}%'.format(int(tn), 100*tn/sum)]]

        my_cmap = seaborn.diverging_palette(131, 4, s=46, l=75,  as_cmap=True) # create colour map from colour palette
        ax4 = seaborn.heatmap([[0, 1],[1, 0]], annot=confusion_matrix, 
                              cmap=my_cmap, cbar=False, fmt='s', 
                              linecolor='black', linewidths=2,
                              annot_kws={"size":14, 'color':'black'}) # creates map using colour map and annotations
        ax4.set_xlabel('Predicted labels', fontsize=16)
        ax4.set_ylabel('True labels', fontsize=16)
        ax4.set_title('Confusion Matrix', fontsize=18)
        ax4.xaxis.set_ticklabels(['Positive', 'Negative'], fontsize=13)
        ax4.yaxis.set_ticklabels(['Positive', 'Negative'], fontsize=13)
    pyplot.show()

def summarise_performance(accuracies, recalls, precisions, dice_hist):
    fig, (ax1, ax2) = pyplot.subplots(1, 2, figsize=(10,5))
    # plot accuracy boxplot
    ax1.boxplot(accuracies)
    ax1.set_title('Accuracy')
    print('Accuracy: ', accuracies)
    # plot dice boxplot
    ax2.boxplot(dice_hist)
    ax2.set_title('Dice')
    print('Dice: ', dice_hist)
    pyplot.show()

def cumulative_graphs():
    COLOUR_LIST = ['blue', 'red', 'green', 'purple', 'orange', 'black', 'pink']
    fig, (ax1, ax2, ax3) = pyplot.subplots(1, 3, figsize=(20,5))
    #fig.suptitle('Learning Rate', fontsize=20)
    # add all series (loss, accuracy and dice)to single graphs
    for i in range(n_splits):
        epochs = range(len(histories[i].history['loss']))
        ax1.plot(epochs, histories[i].history['loss'], 
                    color=COLOUR_LIST[i], label=NEW_LIST[i])
        ax2.plot(epochs, histories[i].history['accuracy'], 
                    color=COLOUR_LIST[i], label=NEW_LIST[i])
        ax3.plot(epochs, histories[i].history['dice'],
                color=COLOUR_LIST[i], label=NEW_LIST[i])
    # loss legend
    ax1.legend(loc='upper right')
    ax1.set_ylabel('Loss', fontsize=16)
    ax1.set_xlabel('Epoch', fontsize=16)
    # accuracy legend
    ax2.set_ylim([0.95, 0.98])
    ax2.legend(loc='lower right')
    ax2.set_ylabel('Accuracy', fontsize=16)
    ax2.set_xlabel('Epoch', fontsize=16)
    # dice legend
    ax3.set_ylim([0.5, 0.7])
    ax3.legend(loc='lower right')
    ax3.set_ylabel('Dice Score', fontsize=16)
    ax3.set_xlabel('Epoch', fontsize=16)
    pyplot.show()

# set constants
IMAGE_SIZE = 128
FILTERS = 2
INPUT_CHANNELS = 3
USE_CACHE = True
BATCH_SIZE = 128
kfold_loop = 1
LEARNING_RATE = 1e-5
DECAY = 1
# initialise lists
histories = list()
accuracies, recalls, precisions, dice_hist = list(), list(), list(), list()
pair_paths = create_pair_paths()
train_paths = pair_paths[:int(0.85*len(pair_paths))]
valid_paths = pair_paths[int(-0.15*len(pair_paths)):]
valid_dataset = create_datasets(valid_paths[:-1])
# used for optimisation to do multiple runs at once
NEW_LIST = [0.80, 0.85, 0.90, 0.95, 1.00]
n_splits = len(NEW_LIST)
if n_splits == 0:
    n_splits = 10
# kfold validation
kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)
for train_ix, test_ix in kfold.split(pair_paths):
    train_paths, test_paths = [], []
    # create new train and split path lists
    for i in train_ix:
        train_paths.append(pair_paths[i])
    for i in test_ix:
        test_paths.append(pair_paths[i])
    # create datasets from lists
    train_dataset = create_datasets(train_paths[:-1])
    test_dataset = create_datasets(test_paths[:-1])
    # set sample image, mask
    for image, mask in test_dataset.take(1):
        sample_image, sample_mask = image, mask
    tensorflow.random.set_seed(1) # set random seed
    # if optimising then set variable to optimise here
    #DECAY = NEW_LIST[kfold_loop-1]
    # set and compile model
    model = unet_model(FILTERS)
    model.compile(optimizer=tensorflow.keras.optimizers.Adam(lr=LEARNING_RATE), 
                loss=dice_BCE_coef_loss, 
                metrics=['accuracy', dice, recall, precision, true_positive, 
                         true_negative, false_positive, false_negative])
    # train model
    model_history = model.fit(train_dataset.repeat(), 
                            epochs=20,
                            steps_per_epoch=len(train_dataset)//BATCH_SIZE,
                            validation_steps=len(test_dataset),
                            validation_data=test_dataset.repeat(),
                            callbacks=[tensorflow.keras.callbacks.LearningRateScheduler(lr_scheduler),
                                       tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
                                       DisplayCallbacks()])
    # append histories
    accuracies.append(model_history.history['accuracy'][-1])
    recalls.append(model_history.history['recall'][-1])
    precisions.append(model_history.history['precision'][-1])
    dice_hist.append(model_history.history['dice'][-1])
    histories.append(model_history)
    kfold_loop = kfold_loop + 1
#display graphs
summarise_diagnostics(histories)
summarise_performance(accuracies, recalls, precisions, dice_hist)
cumulative_graphs()

# save model
model.save('final_model.h5')

#show final predictions
print("Training")
show_predictions(train_dataset, 10)
print("Test")
show_predictions(test_dataset, 5)
print("Validation")
show_predictions(valid_dataset, 5)